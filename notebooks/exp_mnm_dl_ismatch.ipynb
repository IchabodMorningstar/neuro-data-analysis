{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUE_ON_T = 1.0\n",
    "SAMPLE_ON_T = SACCADE_ON_TIME = 3.0\n",
    "MNM_END_TIME = 3.0\n",
    "SUBREGIONS = {'Mid-Dorsal': 'MD', 'Posterior-Ventral': 'PV',\n",
    "              'Anterior-Dorsal': 'AD', 'Posterior-Dorsal': 'PD', 'Anterior-Ventral': 'AV'}\n",
    "\n",
    "def mat_to_df(dg, monkey, area, num_stimuli=1, numeric_cell=False, ):\n",
    "    rows = []\n",
    "    cells = []\n",
    "    for c in range(dg.shape[0]):\n",
    "        c_num = str(c + 1).zfill(3)\n",
    "        cell_name = f'{monkey}-{area}-{c_num}'\n",
    "        # cells.append([c + 1 if numeric_cell else cell_name, monkey, area])\n",
    "        for p in range(dg.shape[1]):\n",
    "            feature_names = dg[c][p][0].dtype.names\n",
    "            for t in range(dg[c, p].shape[1]):\n",
    "                cue_on_t = dg[c, p][0, t]['Cue_onT'][0][0]\n",
    "                ts = dg[c, p][0, t]['TS'].flatten()\n",
    "                if num_stimuli == 1:\n",
    "                    ts = ts - cue_on_t + CUE_ON_T\n",
    "                    ts = np.array([t for t in ts if t >= 0 and t <=\n",
    "                                  SACCADE_ON_TIME + 1.0], dtype=np.float32)\n",
    "                    if (len(ts) > 0):\n",
    "                        rows.append([cell_name, p, t, ts])  # , cue_on_t])\n",
    "                else:\n",
    "                    sample_on_t = dg[c, p][0, t]['Sample_onT'][0][0]\n",
    "                    ts2 = np.array([SAMPLE_ON_T + t - sample_on_t for t in ts if t >\n",
    "                                   sample_on_t and t <= sample_on_t + MNM_END_TIME], dtype=np.float32)\n",
    "                    ts = np.array([t - cue_on_t + CUE_ON_T for t in ts if t >= cue_on_t -\n",
    "                                  CUE_ON_T and t <= cue_on_t + SAMPLE_ON_T - CUE_ON_T], dtype=np.float32)\n",
    "                    ts = np.concatenate((ts, ts2))\n",
    "                    try:\n",
    "                        ismatch = dg[c, p][0, t]['IsMatch'][0][0]\n",
    "                        if len(ts) > 0:\n",
    "                            rows.append([c + 1, p, t, ts, ismatch])\n",
    "                    except:\n",
    "                        pass\n",
    " \n",
    "    df = pd.DataFrame(rows, columns=[\n",
    "                      'cell', 'position', 'trial', 'ts'] + ([] if num_stimuli == 1 else ['ismatch']))\n",
    "    # df = df.set_index(['cell', 'position', 'trial'])\n",
    "    # , pd.DataFrame(cells, columns=['cell', 'monkey', 'area']).set_index('cell')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cells(df, style='monkey-area'):\n",
    "    cells = df.cell.unique()\n",
    "    cells = pd.DataFrame(cells, columns=['cell'])\n",
    "    cells['monkey'] = cells.cell.apply(lambda x: x.split('-')[0])\n",
    "    cells['area'] = cells.cell.apply(lambda x: x.split('-')[1])\n",
    "    if 'subregion' in style:\n",
    "        cells['subregion'] = cells.cell.apply(lambda x: x.split('-')[2])\n",
    "    if 'phase' in style:\n",
    "        cells['phase'] = cells.cell.apply(lambda x: x.split('-')[3])\n",
    "    cells = cells.set_index('cell')\n",
    "    return cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnm_spatial():\n",
    "    asd_mat = loadmat(\n",
    "        '../../../data/MNM_original_data/all_spatial_data.mat')['all_spatial_data']\n",
    "    asi_mat = loadmat(\n",
    "        '../../../data/MNM_original_data/all_spatial_info.mat')['all_spatial_info']\n",
    "\n",
    "    asd_mat = asd_mat[:, :8]\n",
    "    asi_mat = asi_mat[:, [0, 3, 4]]\n",
    "    extract = np.vectorize(lambda x: x[0])\n",
    "    asi_mat = extract(asi_mat)\n",
    "\n",
    "    asi_df = pd.DataFrame(asi_mat, columns=['monkey', 'phase', 'subregion'])\n",
    "    asi_df.monkey = asi_df.monkey.apply(lambda x: x[0:3].upper())\n",
    "    asi_df.subregion = asi_df.subregion.apply(lambda x: SUBREGIONS[x].upper())\n",
    "\n",
    "    asd_df = mat_to_df(asd_mat, '<REPLACE>', 'PFC',\n",
    "                        num_stimuli=2, numeric_cell=True)\n",
    "\n",
    "    df = pd.merge(asd_df, asi_df, left_on='cell', right_index=True)\n",
    "    df['cell'] = df.apply(lambda r: '-'.join([r.monkey, 'PFC',\n",
    "                          str(r.subregion), r.phase, str(r.cell)]), axis=1)\n",
    "    cells = get_cells(df, style='monkey-area-subregion-phase')\n",
    "    df = df.drop(columns=['monkey', 'subregion', 'phase'])\n",
    "\n",
    "    return {'raw_df': df.set_index(['cell', 'position', 'trial']), 'cells': cells}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = load_mnm_spatial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.DataFrame(df_dict['raw_df'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = pd.DataFrame(df_dict['cells'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the index to move 'cell', 'position', and 'trial' back to columns\n",
    "raw_df = raw_df.reset_index()\n",
    "\n",
    "# Verify the change by printing the columns\n",
    "# raw_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell</th>\n",
       "      <th>position</th>\n",
       "      <th>trial</th>\n",
       "      <th>ts</th>\n",
       "      <th>ismatch</th>\n",
       "      <th>monkey</th>\n",
       "      <th>area</th>\n",
       "      <th>subregion</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135700</th>\n",
       "      <td>ADR-PFC-PD-PRE-1077</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.79045, 5.19365]</td>\n",
       "      <td>0</td>\n",
       "      <td>ADR</td>\n",
       "      <td>PFC</td>\n",
       "      <td>PD</td>\n",
       "      <td>PRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314911</th>\n",
       "      <td>NIN-PFC-AD-POST-2895</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>[2.727425, 3.03115, 3.099975, 3.17825, 4.25105]</td>\n",
       "      <td>0</td>\n",
       "      <td>NIN</td>\n",
       "      <td>PFC</td>\n",
       "      <td>AD</td>\n",
       "      <td>POST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214494</th>\n",
       "      <td>ADR-PFC-PV-POST-1891</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>[2.4869, 2.742575, 5.82365]</td>\n",
       "      <td>1</td>\n",
       "      <td>ADR</td>\n",
       "      <td>PFC</td>\n",
       "      <td>PV</td>\n",
       "      <td>POST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        cell  position  trial  \\\n",
       "135700   ADR-PFC-PD-PRE-1077         0      2   \n",
       "314911  NIN-PFC-AD-POST-2895         1     12   \n",
       "214494  ADR-PFC-PV-POST-1891         0     11   \n",
       "\n",
       "                                                     ts  ismatch monkey area  \\\n",
       "135700                               [0.79045, 5.19365]        0    ADR  PFC   \n",
       "314911  [2.727425, 3.03115, 3.099975, 3.17825, 4.25105]        0    NIN  PFC   \n",
       "214494                      [2.4869, 2.742575, 5.82365]        1    ADR  PFC   \n",
       "\n",
       "       subregion phase  \n",
       "135700        PD   PRE  \n",
       "314911        AD  POST  \n",
       "214494        PV  POST  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(raw_df, cells, on='cell', how='inner')\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = df.ts.apply(lambda x: len(x))\n",
    "# lengths.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df\n",
    "filtered_df['ts_length'] = filtered_df['ts'].apply(len)\n",
    "average_ts_length = filtered_df.groupby('cell')['ts_length'].mean().reset_index()\n",
    "sum_ts_length = filtered_df.groupby('cell')['ts_length'].sum().reset_index()\n",
    "use_cell = average_ts_length[(average_ts_length.ts_length >= 100) & (sum_ts_length.ts_length >= 500)].cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12980, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = filtered_df[filtered_df['cell'].isin(use_cell)]\n",
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6497, 10), (6483, 10))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df = filtered_df[filtered_df.phase == \"PRE\"]\n",
    "post_df = filtered_df[filtered_df.phase == \"POST\"]\n",
    "pre_df.shape, post_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_soft_one_hot(pos):\n",
    "    return torch.tensor(\n",
    "        [\n",
    "            0.6 if pos == x else (0.2 if abs(x - pos) ==\n",
    "                                  1 or abs(x - pos) == 7 else 0)\n",
    "            for x in range(1, 9)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def create_y(l, func):\n",
    "    return torch.hstack(\n",
    "        [\n",
    "            torch.stack([func(y) for y in l]),\n",
    "            torch.tensor(l).unsqueeze(1),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def window_data(df, NUM_OF_SAMPLES, WINDOW_STRIDE, WINDOW_WIDTH, NUM_POSITIONS, NUM_OF_CELLS):\n",
    "    df = df.sort_values(['cell', 'ismatch']).reset_index(drop=True)\n",
    "\n",
    "    window_data = []\n",
    "    for start in np.arange(1, 3.5, WINDOW_STRIDE):\n",
    "        window_data.append(df.ts.apply(lambda x: (\n",
    "            (x >= start) & (x < start + WINDOW_WIDTH)).sum()).array)\n",
    "\n",
    "    window_d_t = torch.tensor(window_data).T\n",
    "    BINS = window_d_t.shape[1]\n",
    "\n",
    "    X = torch.zeros((NUM_OF_SAMPLES * NUM_POSITIONS,\n",
    "                    NUM_OF_CELLS * BINS), dtype=torch.float32)\n",
    "    y = []\n",
    "    for p in range(1, NUM_POSITIONS+1):\n",
    "        y += [p] * NUM_OF_SAMPLES\n",
    "        cells_idxs = df[df.position == p-1].groupby(\"cell\", sort=False).apply(\n",
    "            lambda x: x.index)\n",
    "        for i, cell_idxs in enumerate(cells_idxs):\n",
    "            idxs = np.random.choice(cell_idxs, NUM_OF_SAMPLES)\n",
    "            for j, idx in enumerate(idxs):\n",
    "                X[(p - 1) * NUM_OF_SAMPLES + j, i *\n",
    "                  BINS: (i + 1) * BINS] = window_d_t[idx]\n",
    "\n",
    "    return X, create_y(y, get_soft_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "NUM_OF_CELLS = pre_df.cell.nunique()\n",
    "NUM_OF_SAMPLES = 40\n",
    "WINDOW_STRIDE = 0.4\n",
    "WINDOW_WIDTH = 0.4\n",
    "NUM_POSITIONS = 2\n",
    "\n",
    "X, y = window_data(pre_df, NUM_OF_SAMPLES, WINDOW_STRIDE, WINDOW_WIDTH, NUM_POSITIONS, NUM_OF_CELLS)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 378])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 378]), torch.Size([16, 378]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 9])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 2.7215, Accuracy: 51.56%\n",
      "Epoch [2/20], Loss: 1.3895, Accuracy: 59.38%\n",
      "Epoch [3/20], Loss: 0.7394, Accuracy: 78.12%\n",
      "Epoch [4/20], Loss: 0.5624, Accuracy: 81.25%\n",
      "Epoch [5/20], Loss: 0.5582, Accuracy: 79.69%\n",
      "Epoch [6/20], Loss: 0.1844, Accuracy: 90.62%\n",
      "Epoch [7/20], Loss: 0.2455, Accuracy: 89.06%\n",
      "Epoch [8/20], Loss: 0.1695, Accuracy: 92.19%\n",
      "Epoch [9/20], Loss: 0.0327, Accuracy: 100.00%\n",
      "Epoch [10/20], Loss: 0.0851, Accuracy: 95.31%\n",
      "Epoch [11/20], Loss: 0.0765, Accuracy: 98.44%\n",
      "Epoch [12/20], Loss: 0.0236, Accuracy: 100.00%\n",
      "Epoch [13/20], Loss: 0.0157, Accuracy: 100.00%\n",
      "Epoch [14/20], Loss: 0.0301, Accuracy: 100.00%\n",
      "Epoch [15/20], Loss: 0.0276, Accuracy: 100.00%\n",
      "Epoch [16/20], Loss: 0.0126, Accuracy: 100.00%\n",
      "Epoch [17/20], Loss: 0.0064, Accuracy: 100.00%\n",
      "Epoch [18/20], Loss: 0.0047, Accuracy: 100.00%\n",
      "Epoch [19/20], Loss: 0.0056, Accuracy: 100.00%\n",
      "Epoch [20/20], Loss: 0.0073, Accuracy: 100.00%\n",
      "Training Finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "y_train_labels = y_train[:, -1].long()\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "y_test_labels = y_test[:, -1].long()\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test_labels)\n",
    "test_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class SimpleFNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SimpleFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "input_size = X.shape[1]\n",
    "num_classes = y_train_labels.max().item() + 1\n",
    "\n",
    "model = SimpleFNN(input_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "print('Training Finished.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0123, Test Accuracy: 100.00%\n",
      "Final Test Loss: 0.0123, Final Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
    "    return avg_test_loss, accuracy\n",
    "\n",
    "# Evaluate the model after training\n",
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "\n",
    "print(f'Final Test Loss: {test_loss:.4f}, Final Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 1.0990, Accuracy: 51.67%\n",
      "Epoch [2/30], Loss: 0.8720, Accuracy: 51.67%\n",
      "Epoch [3/30], Loss: 0.7603, Accuracy: 61.67%\n",
      "Epoch [4/30], Loss: 0.6785, Accuracy: 70.00%\n",
      "Epoch [5/30], Loss: 0.6282, Accuracy: 78.33%\n",
      "Epoch [6/30], Loss: 0.6001, Accuracy: 76.67%\n",
      "Epoch [7/30], Loss: 0.5441, Accuracy: 76.67%\n",
      "Epoch [8/30], Loss: 0.4990, Accuracy: 91.67%\n",
      "Epoch [9/30], Loss: 0.4376, Accuracy: 93.33%\n",
      "Epoch [10/30], Loss: 0.3727, Accuracy: 93.33%\n",
      "Epoch [11/30], Loss: 0.3107, Accuracy: 96.67%\n",
      "Epoch [12/30], Loss: 0.2439, Accuracy: 98.33%\n",
      "Epoch [13/30], Loss: 0.1886, Accuracy: 100.00%\n",
      "Epoch [14/30], Loss: 0.1446, Accuracy: 100.00%\n",
      "Epoch [15/30], Loss: 0.1053, Accuracy: 100.00%\n",
      "Epoch [16/30], Loss: 0.0734, Accuracy: 100.00%\n",
      "Epoch [17/30], Loss: 0.0514, Accuracy: 100.00%\n",
      "Epoch [18/30], Loss: 0.0358, Accuracy: 100.00%\n",
      "Epoch [19/30], Loss: 0.0242, Accuracy: 100.00%\n",
      "Epoch [20/30], Loss: 0.0161, Accuracy: 100.00%\n",
      "Epoch [21/30], Loss: 0.0116, Accuracy: 100.00%\n",
      "Epoch [22/30], Loss: 0.0086, Accuracy: 100.00%\n",
      "Epoch [23/30], Loss: 0.0060, Accuracy: 100.00%\n",
      "Epoch [24/30], Loss: 0.0046, Accuracy: 100.00%\n",
      "Epoch [25/30], Loss: 0.0036, Accuracy: 100.00%\n",
      "Epoch [26/30], Loss: 0.0028, Accuracy: 100.00%\n",
      "Epoch [27/30], Loss: 0.0023, Accuracy: 100.00%\n",
      "Epoch [28/30], Loss: 0.0019, Accuracy: 100.00%\n",
      "Epoch [29/30], Loss: 0.0016, Accuracy: 100.00%\n",
      "Epoch [30/30], Loss: 0.0014, Accuracy: 100.00%\n",
      "Training Finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "y_train_labels = y_train[:, -1].long()\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "y_test_labels = y_test[:, -1].long()\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test_labels)\n",
    "test_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_cells, bins, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_cells, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "BINS = 7\n",
    "num_cells = X.shape[1] // BINS\n",
    "bins = BINS\n",
    "num_classes = y_train_labels.max().item() + 1\n",
    "\n",
    "model = SimpleCNN(num_cells, bins, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.view(inputs.size(0), num_cells, bins)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "print('Training Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0013, Test Accuracy: 100.00%\n",
      "Final Test Loss: 0.0013, Final Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, criterion, num_cells, bins):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.view(inputs.size(0), num_cells, bins)  # Reshape the inputs to match the CNN input size\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
    "    return avg_test_loss, accuracy\n",
    "\n",
    "# Evaluate the model after training\n",
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, num_cells, bins)\n",
    "\n",
    "print(f'Final Test Loss: {test_loss:.4f}, Final Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.0499, Accuracy: 46.67%\n",
      "Epoch [2/100], Loss: 1.0141, Accuracy: 66.67%\n",
      "Epoch [3/100], Loss: 0.9781, Accuracy: 56.67%\n",
      "Epoch [4/100], Loss: 0.9396, Accuracy: 53.33%\n",
      "Epoch [5/100], Loss: 0.9014, Accuracy: 53.33%\n",
      "Epoch [6/100], Loss: 0.8654, Accuracy: 53.33%\n",
      "Epoch [7/100], Loss: 0.8267, Accuracy: 53.33%\n",
      "Epoch [8/100], Loss: 0.8000, Accuracy: 53.33%\n",
      "Epoch [9/100], Loss: 0.7755, Accuracy: 53.33%\n",
      "Epoch [10/100], Loss: 0.7581, Accuracy: 53.33%\n",
      "Epoch [11/100], Loss: 0.7435, Accuracy: 53.33%\n",
      "Epoch [12/100], Loss: 0.7334, Accuracy: 53.33%\n",
      "Epoch [13/100], Loss: 0.7246, Accuracy: 53.33%\n",
      "Epoch [14/100], Loss: 0.7166, Accuracy: 53.33%\n",
      "Epoch [15/100], Loss: 0.7091, Accuracy: 56.67%\n",
      "Epoch [16/100], Loss: 0.7029, Accuracy: 56.67%\n",
      "Epoch [17/100], Loss: 0.6973, Accuracy: 56.67%\n",
      "Epoch [18/100], Loss: 0.6907, Accuracy: 56.67%\n",
      "Epoch [19/100], Loss: 0.6891, Accuracy: 55.00%\n",
      "Epoch [20/100], Loss: 0.6875, Accuracy: 55.00%\n",
      "Epoch [21/100], Loss: 0.6733, Accuracy: 66.67%\n",
      "Epoch [22/100], Loss: 0.6725, Accuracy: 81.67%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100], Loss: 0.6573, Accuracy: 71.67%\n",
      "Epoch [24/100], Loss: 0.6511, Accuracy: 63.33%\n",
      "Epoch [25/100], Loss: 0.6403, Accuracy: 70.00%\n",
      "Epoch [26/100], Loss: 0.6288, Accuracy: 81.67%\n",
      "Epoch [27/100], Loss: 0.6143, Accuracy: 78.33%\n",
      "Epoch [28/100], Loss: 0.5986, Accuracy: 78.33%\n",
      "Epoch [29/100], Loss: 0.5877, Accuracy: 83.33%\n",
      "Epoch [30/100], Loss: 0.5898, Accuracy: 73.33%\n",
      "Epoch [31/100], Loss: 0.5465, Accuracy: 83.33%\n",
      "Epoch [32/100], Loss: 0.5506, Accuracy: 88.33%\n",
      "Epoch [33/100], Loss: 0.5591, Accuracy: 71.67%\n",
      "Epoch [34/100], Loss: 0.5127, Accuracy: 88.33%\n",
      "Epoch [35/100], Loss: 0.5066, Accuracy: 91.67%\n",
      "Epoch [36/100], Loss: 0.4760, Accuracy: 88.33%\n",
      "Epoch [37/100], Loss: 0.4677, Accuracy: 88.33%\n",
      "Epoch [38/100], Loss: 0.4298, Accuracy: 93.33%\n",
      "Epoch [39/100], Loss: 0.4271, Accuracy: 91.67%\n",
      "Epoch [40/100], Loss: 0.3949, Accuracy: 95.00%\n",
      "Epoch [41/100], Loss: 0.3620, Accuracy: 96.67%\n",
      "Epoch [42/100], Loss: 0.3483, Accuracy: 96.67%\n",
      "Epoch [43/100], Loss: 0.3216, Accuracy: 96.67%\n",
      "Epoch [44/100], Loss: 0.2856, Accuracy: 100.00%\n",
      "Epoch [45/100], Loss: 0.2683, Accuracy: 100.00%\n",
      "Epoch [46/100], Loss: 0.2601, Accuracy: 100.00%\n",
      "Epoch [47/100], Loss: 0.2427, Accuracy: 100.00%\n",
      "Epoch [48/100], Loss: 0.2237, Accuracy: 100.00%\n",
      "Epoch [49/100], Loss: 0.2148, Accuracy: 100.00%\n",
      "Epoch [50/100], Loss: 0.2203, Accuracy: 100.00%\n",
      "Epoch [51/100], Loss: 0.2050, Accuracy: 98.33%\n",
      "Epoch [52/100], Loss: 0.1802, Accuracy: 100.00%\n",
      "Epoch [53/100], Loss: 0.1894, Accuracy: 100.00%\n",
      "Epoch [54/100], Loss: 0.1590, Accuracy: 100.00%\n",
      "Epoch [55/100], Loss: 0.1801, Accuracy: 100.00%\n",
      "Epoch [56/100], Loss: 0.1431, Accuracy: 100.00%\n",
      "Epoch [57/100], Loss: 0.1653, Accuracy: 100.00%\n",
      "Epoch [58/100], Loss: 0.1235, Accuracy: 100.00%\n",
      "Epoch [59/100], Loss: 0.1423, Accuracy: 98.33%\n",
      "Epoch [60/100], Loss: 0.1141, Accuracy: 100.00%\n",
      "Epoch [61/100], Loss: 0.1090, Accuracy: 100.00%\n",
      "Epoch [62/100], Loss: 0.1128, Accuracy: 100.00%\n",
      "Epoch [63/100], Loss: 0.0925, Accuracy: 100.00%\n",
      "Epoch [64/100], Loss: 0.0929, Accuracy: 100.00%\n",
      "Epoch [65/100], Loss: 0.0888, Accuracy: 100.00%\n",
      "Epoch [66/100], Loss: 0.0796, Accuracy: 100.00%\n",
      "Epoch [67/100], Loss: 0.0749, Accuracy: 100.00%\n",
      "Epoch [68/100], Loss: 0.0725, Accuracy: 100.00%\n",
      "Epoch [69/100], Loss: 0.0691, Accuracy: 100.00%\n",
      "Epoch [70/100], Loss: 0.0646, Accuracy: 100.00%\n",
      "Epoch [71/100], Loss: 0.0605, Accuracy: 100.00%\n",
      "Epoch [72/100], Loss: 0.0575, Accuracy: 100.00%\n",
      "Epoch [73/100], Loss: 0.0548, Accuracy: 100.00%\n",
      "Epoch [74/100], Loss: 0.0526, Accuracy: 100.00%\n",
      "Epoch [75/100], Loss: 0.0501, Accuracy: 100.00%\n",
      "Epoch [76/100], Loss: 0.0477, Accuracy: 100.00%\n",
      "Epoch [77/100], Loss: 0.0456, Accuracy: 100.00%\n",
      "Epoch [78/100], Loss: 0.0438, Accuracy: 100.00%\n",
      "Epoch [79/100], Loss: 0.0421, Accuracy: 100.00%\n",
      "Epoch [80/100], Loss: 0.0404, Accuracy: 100.00%\n",
      "Epoch [81/100], Loss: 0.0389, Accuracy: 100.00%\n",
      "Epoch [82/100], Loss: 0.0371, Accuracy: 100.00%\n",
      "Epoch [83/100], Loss: 0.0355, Accuracy: 100.00%\n",
      "Epoch [84/100], Loss: 0.0342, Accuracy: 100.00%\n",
      "Epoch [85/100], Loss: 0.0325, Accuracy: 100.00%\n",
      "Epoch [86/100], Loss: 0.0309, Accuracy: 100.00%\n",
      "Epoch [87/100], Loss: 0.0292, Accuracy: 100.00%\n",
      "Epoch [88/100], Loss: 0.0275, Accuracy: 100.00%\n",
      "Epoch [89/100], Loss: 0.0260, Accuracy: 100.00%\n",
      "Epoch [90/100], Loss: 0.0245, Accuracy: 100.00%\n",
      "Epoch [91/100], Loss: 0.0232, Accuracy: 100.00%\n",
      "Epoch [92/100], Loss: 0.0220, Accuracy: 100.00%\n",
      "Epoch [93/100], Loss: 0.0209, Accuracy: 100.00%\n",
      "Epoch [94/100], Loss: 0.0199, Accuracy: 100.00%\n",
      "Epoch [95/100], Loss: 0.0191, Accuracy: 100.00%\n",
      "Epoch [96/100], Loss: 0.0182, Accuracy: 100.00%\n",
      "Epoch [97/100], Loss: 0.0175, Accuracy: 100.00%\n",
      "Epoch [98/100], Loss: 0.0168, Accuracy: 100.00%\n",
      "Epoch [99/100], Loss: 0.0162, Accuracy: 100.00%\n",
      "Epoch [100/100], Loss: 0.0156, Accuracy: 100.00%\n",
      "Training Finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "y_train_labels = y_train[:, -1].long()\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "y_test_labels = y_test[:, -1].long()\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test_labels)\n",
    "test_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=num_features, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.relu(x[:, -1, :])\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "num_features = BINS\n",
    "sequence_length = len(pre_df.cell.unique())\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "num_classes = y_train_labels.max().item() + 1\n",
    "\n",
    "model = LSTMModel(num_features, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.view(inputs.size(0), sequence_length, num_features)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "print('Training Finished.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0152, Test Accuracy: 100.00%\n",
      "Final Test Loss: 0.0152, Final Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function for LSTM model\n",
    "def evaluate_model(model, test_loader, criterion, sequence_length, num_features):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.view(inputs.size(0), sequence_length, num_features)  # Reshape inputs\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
    "    return avg_test_loss, accuracy\n",
    "\n",
    "# Evaluate the model after training\n",
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, sequence_length, num_features)\n",
    "\n",
    "print(f'Final Test Loss: {test_loss:.4f}, Final Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "NUM_OF_CELLS = post_df.cell.nunique()\n",
    "NUM_OF_SAMPLES = 40\n",
    "WINDOW_STRIDE = 0.4\n",
    "WINDOW_WIDTH = 0.4\n",
    "NUM_POSITIONS = 2\n",
    "\n",
    "X, y = window_data(post_df, NUM_OF_SAMPLES, WINDOW_STRIDE, WINDOW_WIDTH, NUM_POSITIONS, NUM_OF_CELLS)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.6060, Accuracy: 31.25%\n",
      "Epoch [2/20], Loss: 0.6778, Accuracy: 71.88%\n",
      "Epoch [3/20], Loss: 0.4185, Accuracy: 82.81%\n",
      "Epoch [4/20], Loss: 0.2822, Accuracy: 84.38%\n",
      "Epoch [5/20], Loss: 0.1491, Accuracy: 93.75%\n",
      "Epoch [6/20], Loss: 0.2013, Accuracy: 89.06%\n",
      "Epoch [7/20], Loss: 0.0516, Accuracy: 100.00%\n",
      "Epoch [8/20], Loss: 0.1778, Accuracy: 92.19%\n",
      "Epoch [9/20], Loss: 0.0327, Accuracy: 100.00%\n",
      "Epoch [10/20], Loss: 0.0770, Accuracy: 98.44%\n",
      "Epoch [11/20], Loss: 0.0677, Accuracy: 96.88%\n",
      "Epoch [12/20], Loss: 0.0080, Accuracy: 100.00%\n",
      "Epoch [13/20], Loss: 0.0159, Accuracy: 100.00%\n",
      "Epoch [14/20], Loss: 0.0342, Accuracy: 100.00%\n",
      "Epoch [15/20], Loss: 0.0149, Accuracy: 100.00%\n",
      "Epoch [16/20], Loss: 0.0041, Accuracy: 100.00%\n",
      "Epoch [17/20], Loss: 0.0028, Accuracy: 100.00%\n",
      "Epoch [18/20], Loss: 0.0051, Accuracy: 100.00%\n",
      "Epoch [19/20], Loss: 0.0082, Accuracy: 100.00%\n",
      "Epoch [20/20], Loss: 0.0076, Accuracy: 100.00%\n",
      "Training Finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "y_train_labels = y_train[:, -1].long()\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "y_test_labels = y_test[:, -1].long()\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test_labels)\n",
    "test_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class SimpleFNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SimpleFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "input_size = X.shape[1]\n",
    "num_classes = y_train_labels.max().item() + 1\n",
    "\n",
    "model = SimpleFNN(input_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "print('Training Finished.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0053, Test Accuracy: 100.00%\n",
      "Final Test Loss: 0.0053, Final Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
    "    return avg_test_loss, accuracy\n",
    "\n",
    "# Evaluate the model after training\n",
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "\n",
    "print(f'Final Test Loss: {test_loss:.4f}, Final Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 0.9554, Accuracy: 51.56%\n",
      "Epoch [2/30], Loss: 0.7760, Accuracy: 51.56%\n",
      "Epoch [3/30], Loss: 0.6703, Accuracy: 57.81%\n",
      "Epoch [4/30], Loss: 0.6314, Accuracy: 75.00%\n",
      "Epoch [5/30], Loss: 0.5592, Accuracy: 81.25%\n",
      "Epoch [6/30], Loss: 0.5155, Accuracy: 73.44%\n",
      "Epoch [7/30], Loss: 0.4669, Accuracy: 90.62%\n",
      "Epoch [8/30], Loss: 0.4187, Accuracy: 90.62%\n",
      "Epoch [9/30], Loss: 0.3577, Accuracy: 90.62%\n",
      "Epoch [10/30], Loss: 0.3018, Accuracy: 95.31%\n",
      "Epoch [11/30], Loss: 0.2609, Accuracy: 96.88%\n",
      "Epoch [12/30], Loss: 0.2141, Accuracy: 96.88%\n",
      "Epoch [13/30], Loss: 0.1777, Accuracy: 98.44%\n",
      "Epoch [14/30], Loss: 0.1381, Accuracy: 98.44%\n",
      "Epoch [15/30], Loss: 0.1111, Accuracy: 98.44%\n",
      "Epoch [16/30], Loss: 0.0884, Accuracy: 100.00%\n",
      "Epoch [17/30], Loss: 0.0644, Accuracy: 100.00%\n",
      "Epoch [18/30], Loss: 0.0511, Accuracy: 100.00%\n",
      "Epoch [19/30], Loss: 0.0369, Accuracy: 100.00%\n",
      "Epoch [20/30], Loss: 0.0278, Accuracy: 100.00%\n",
      "Epoch [21/30], Loss: 0.0226, Accuracy: 100.00%\n",
      "Epoch [22/30], Loss: 0.0142, Accuracy: 100.00%\n",
      "Epoch [23/30], Loss: 0.0130, Accuracy: 100.00%\n",
      "Epoch [24/30], Loss: 0.0097, Accuracy: 100.00%\n",
      "Epoch [25/30], Loss: 0.0071, Accuracy: 100.00%\n",
      "Epoch [26/30], Loss: 0.0060, Accuracy: 100.00%\n",
      "Epoch [27/30], Loss: 0.0050, Accuracy: 100.00%\n",
      "Epoch [28/30], Loss: 0.0039, Accuracy: 100.00%\n",
      "Epoch [29/30], Loss: 0.0033, Accuracy: 100.00%\n",
      "Epoch [30/30], Loss: 0.0029, Accuracy: 100.00%\n",
      "Training Finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "y_train_labels = y_train[:, -1].long()\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "y_test_labels = y_test[:, -1].long()\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test_labels)\n",
    "test_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_cells, bins, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_cells, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "BINS = 7\n",
    "num_cells = X.shape[1] // BINS\n",
    "bins = BINS\n",
    "num_classes = y_train_labels.max().item() + 1\n",
    "\n",
    "model = SimpleCNN(num_cells, bins, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.view(inputs.size(0), num_cells, bins)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "print('Training Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0027, Test Accuracy: 100.00%\n",
      "Final Test Loss: 0.0027, Final Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, criterion, num_cells, bins):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.view(inputs.size(0), num_cells, bins)  # Reshape the inputs to match the CNN input size\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
    "    return avg_test_loss, accuracy\n",
    "\n",
    "# Evaluate the model after training\n",
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, num_cells, bins)\n",
    "\n",
    "print(f'Final Test Loss: {test_loss:.4f}, Final Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.1021, Accuracy: 26.56%\n",
      "Epoch [2/100], Loss: 1.0546, Accuracy: 46.88%\n",
      "Epoch [3/100], Loss: 1.0074, Accuracy: 46.88%\n",
      "Epoch [4/100], Loss: 0.9634, Accuracy: 46.88%\n",
      "Epoch [5/100], Loss: 0.9235, Accuracy: 46.88%\n",
      "Epoch [6/100], Loss: 0.8834, Accuracy: 46.88%\n",
      "Epoch [7/100], Loss: 0.8445, Accuracy: 46.88%\n",
      "Epoch [8/100], Loss: 0.8129, Accuracy: 46.88%\n",
      "Epoch [9/100], Loss: 0.7825, Accuracy: 46.88%\n",
      "Epoch [10/100], Loss: 0.7627, Accuracy: 54.69%\n",
      "Epoch [11/100], Loss: 0.7474, Accuracy: 54.69%\n",
      "Epoch [12/100], Loss: 0.7356, Accuracy: 53.12%\n",
      "Epoch [13/100], Loss: 0.7273, Accuracy: 53.12%\n",
      "Epoch [14/100], Loss: 0.7209, Accuracy: 53.12%\n",
      "Epoch [15/100], Loss: 0.7155, Accuracy: 53.12%\n",
      "Epoch [16/100], Loss: 0.7105, Accuracy: 53.12%\n",
      "Epoch [17/100], Loss: 0.7059, Accuracy: 53.12%\n",
      "Epoch [18/100], Loss: 0.7024, Accuracy: 53.12%\n",
      "Epoch [19/100], Loss: 0.7005, Accuracy: 54.69%\n",
      "Epoch [20/100], Loss: 0.6953, Accuracy: 62.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100], Loss: 0.6914, Accuracy: 64.06%\n",
      "Epoch [22/100], Loss: 0.6866, Accuracy: 57.81%\n",
      "Epoch [23/100], Loss: 0.6862, Accuracy: 54.69%\n",
      "Epoch [24/100], Loss: 0.6787, Accuracy: 56.25%\n",
      "Epoch [25/100], Loss: 0.6706, Accuracy: 76.56%\n",
      "Epoch [26/100], Loss: 0.6640, Accuracy: 71.88%\n",
      "Epoch [27/100], Loss: 0.6567, Accuracy: 70.31%\n",
      "Epoch [28/100], Loss: 0.6475, Accuracy: 76.56%\n",
      "Epoch [29/100], Loss: 0.6396, Accuracy: 78.12%\n",
      "Epoch [30/100], Loss: 0.6302, Accuracy: 78.12%\n",
      "Epoch [31/100], Loss: 0.6190, Accuracy: 73.44%\n",
      "Epoch [32/100], Loss: 0.6045, Accuracy: 76.56%\n",
      "Epoch [33/100], Loss: 0.5928, Accuracy: 81.25%\n",
      "Epoch [34/100], Loss: 0.5787, Accuracy: 81.25%\n",
      "Epoch [35/100], Loss: 0.5601, Accuracy: 79.69%\n",
      "Epoch [36/100], Loss: 0.5495, Accuracy: 79.69%\n",
      "Epoch [37/100], Loss: 0.5263, Accuracy: 81.25%\n",
      "Epoch [38/100], Loss: 0.5048, Accuracy: 81.25%\n",
      "Epoch [39/100], Loss: 0.4824, Accuracy: 85.94%\n",
      "Epoch [40/100], Loss: 0.4837, Accuracy: 81.25%\n",
      "Epoch [41/100], Loss: 0.4384, Accuracy: 85.94%\n",
      "Epoch [42/100], Loss: 0.4298, Accuracy: 90.62%\n",
      "Epoch [43/100], Loss: 0.3934, Accuracy: 89.06%\n",
      "Epoch [44/100], Loss: 0.3784, Accuracy: 89.06%\n",
      "Epoch [45/100], Loss: 0.3531, Accuracy: 90.62%\n",
      "Epoch [46/100], Loss: 0.3298, Accuracy: 93.75%\n",
      "Epoch [47/100], Loss: 0.3111, Accuracy: 93.75%\n",
      "Epoch [48/100], Loss: 0.2999, Accuracy: 96.88%\n",
      "Epoch [49/100], Loss: 0.2990, Accuracy: 92.19%\n",
      "Epoch [50/100], Loss: 0.2635, Accuracy: 93.75%\n",
      "Epoch [51/100], Loss: 0.2484, Accuracy: 98.44%\n",
      "Epoch [52/100], Loss: 0.2257, Accuracy: 96.88%\n",
      "Epoch [53/100], Loss: 0.2071, Accuracy: 98.44%\n",
      "Epoch [54/100], Loss: 0.1798, Accuracy: 98.44%\n",
      "Epoch [55/100], Loss: 0.1751, Accuracy: 98.44%\n",
      "Epoch [56/100], Loss: 0.1507, Accuracy: 100.00%\n",
      "Epoch [57/100], Loss: 0.1512, Accuracy: 100.00%\n",
      "Epoch [58/100], Loss: 0.1247, Accuracy: 100.00%\n",
      "Epoch [59/100], Loss: 0.1218, Accuracy: 100.00%\n",
      "Epoch [60/100], Loss: 0.1054, Accuracy: 100.00%\n",
      "Epoch [61/100], Loss: 0.1055, Accuracy: 100.00%\n",
      "Epoch [62/100], Loss: 0.0910, Accuracy: 100.00%\n",
      "Epoch [63/100], Loss: 0.0866, Accuracy: 100.00%\n",
      "Epoch [64/100], Loss: 0.0728, Accuracy: 100.00%\n",
      "Epoch [65/100], Loss: 0.0728, Accuracy: 100.00%\n",
      "Epoch [66/100], Loss: 0.0617, Accuracy: 100.00%\n",
      "Epoch [67/100], Loss: 0.0601, Accuracy: 100.00%\n",
      "Epoch [68/100], Loss: 0.0528, Accuracy: 100.00%\n",
      "Epoch [69/100], Loss: 0.0497, Accuracy: 100.00%\n",
      "Epoch [70/100], Loss: 0.0473, Accuracy: 100.00%\n",
      "Epoch [71/100], Loss: 0.0427, Accuracy: 100.00%\n",
      "Epoch [72/100], Loss: 0.0409, Accuracy: 100.00%\n",
      "Epoch [73/100], Loss: 0.0377, Accuracy: 100.00%\n",
      "Epoch [74/100], Loss: 0.0355, Accuracy: 100.00%\n",
      "Epoch [75/100], Loss: 0.0340, Accuracy: 100.00%\n",
      "Epoch [76/100], Loss: 0.0319, Accuracy: 100.00%\n",
      "Epoch [77/100], Loss: 0.0304, Accuracy: 100.00%\n",
      "Epoch [78/100], Loss: 0.0292, Accuracy: 100.00%\n",
      "Epoch [79/100], Loss: 0.0278, Accuracy: 100.00%\n",
      "Epoch [80/100], Loss: 0.0265, Accuracy: 100.00%\n",
      "Epoch [81/100], Loss: 0.0255, Accuracy: 100.00%\n",
      "Epoch [82/100], Loss: 0.0245, Accuracy: 100.00%\n",
      "Epoch [83/100], Loss: 0.0236, Accuracy: 100.00%\n",
      "Epoch [84/100], Loss: 0.0227, Accuracy: 100.00%\n",
      "Epoch [85/100], Loss: 0.0219, Accuracy: 100.00%\n",
      "Epoch [86/100], Loss: 0.0211, Accuracy: 100.00%\n",
      "Epoch [87/100], Loss: 0.0204, Accuracy: 100.00%\n",
      "Epoch [88/100], Loss: 0.0197, Accuracy: 100.00%\n",
      "Epoch [89/100], Loss: 0.0191, Accuracy: 100.00%\n",
      "Epoch [90/100], Loss: 0.0184, Accuracy: 100.00%\n",
      "Epoch [91/100], Loss: 0.0179, Accuracy: 100.00%\n",
      "Epoch [92/100], Loss: 0.0173, Accuracy: 100.00%\n",
      "Epoch [93/100], Loss: 0.0168, Accuracy: 100.00%\n",
      "Epoch [94/100], Loss: 0.0163, Accuracy: 100.00%\n",
      "Epoch [95/100], Loss: 0.0158, Accuracy: 100.00%\n",
      "Epoch [96/100], Loss: 0.0153, Accuracy: 100.00%\n",
      "Epoch [97/100], Loss: 0.0148, Accuracy: 100.00%\n",
      "Epoch [98/100], Loss: 0.0144, Accuracy: 100.00%\n",
      "Epoch [99/100], Loss: 0.0140, Accuracy: 100.00%\n",
      "Epoch [100/100], Loss: 0.0136, Accuracy: 100.00%\n",
      "Training Finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "y_train_labels = y_train[:, -1].long()\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "y_test_labels = y_test[:, -1].long()\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test_labels)\n",
    "test_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=num_features, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.relu(x[:, -1, :])\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "num_features = BINS\n",
    "sequence_length = len(post_df.cell.unique())\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "num_classes = y_train_labels.max().item() + 1\n",
    "\n",
    "model = LSTMModel(num_features, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.view(inputs.size(0), sequence_length, num_features)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "print('Training Finished.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0133, Test Accuracy: 100.00%\n",
      "Final Test Loss: 0.0133, Final Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function for LSTM model\n",
    "def evaluate_model(model, test_loader, criterion, sequence_length, num_features):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.view(inputs.size(0), sequence_length, num_features)  # Reshape inputs\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
    "    return avg_test_loss, accuracy\n",
    "\n",
    "# Evaluate the model after training\n",
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, sequence_length, num_features)\n",
    "\n",
    "print(f'Final Test Loss: {test_loss:.4f}, Final Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
