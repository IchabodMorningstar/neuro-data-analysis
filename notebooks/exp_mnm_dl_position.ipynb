{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUE_ON_T = 1.0\n",
    "SAMPLE_ON_T = SACCADE_ON_TIME = 3.0\n",
    "MNM_END_TIME = 3.0\n",
    "SUBREGIONS = {'Mid-Dorsal': 'MD', 'Posterior-Ventral': 'PV',\n",
    "              'Anterior-Dorsal': 'AD', 'Posterior-Dorsal': 'PD', 'Anterior-Ventral': 'AV'}\n",
    "\n",
    "def mat_to_df(dg, monkey, area, num_stimuli=1, numeric_cell=False, ):\n",
    "    rows = []\n",
    "    cells = []\n",
    "    for c in range(dg.shape[0]):\n",
    "        c_num = str(c + 1).zfill(3)\n",
    "        cell_name = f'{monkey}-{area}-{c_num}'\n",
    "        # cells.append([c + 1 if numeric_cell else cell_name, monkey, area])\n",
    "        for p in range(dg.shape[1]):\n",
    "            feature_names = dg[c][p][0].dtype.names\n",
    "            for t in range(dg[c, p].shape[1]):\n",
    "                cue_on_t = dg[c, p][0, t]['Cue_onT'][0][0]\n",
    "                ts = dg[c, p][0, t]['TS'].flatten()\n",
    "                if num_stimuli == 1:\n",
    "                    ts = ts - cue_on_t + CUE_ON_T\n",
    "                    ts = np.array([t for t in ts if t >= 0 and t <=\n",
    "                                  SACCADE_ON_TIME + 1.0], dtype=np.float32)\n",
    "                    if (len(ts) > 0):\n",
    "                        rows.append([cell_name, p, t, ts])  # , cue_on_t])\n",
    "                else:\n",
    "                    sample_on_t = dg[c, p][0, t]['Sample_onT'][0][0]\n",
    "                    ts2 = np.array([SAMPLE_ON_T + t - sample_on_t for t in ts if t >\n",
    "                                   sample_on_t and t <= sample_on_t + MNM_END_TIME], dtype=np.float32)\n",
    "                    ts = np.array([t - cue_on_t + CUE_ON_T for t in ts if t >= cue_on_t -\n",
    "                                  CUE_ON_T and t <= cue_on_t + SAMPLE_ON_T - CUE_ON_T], dtype=np.float32)\n",
    "                    ts = np.concatenate((ts, ts2))\n",
    "                    try:\n",
    "                        ismatch = dg[c, p][0, t]['IsMatch'][0][0]\n",
    "                        if len(ts) > 0:\n",
    "                            rows.append([c + 1, p, t, ts, ismatch])\n",
    "                    except:\n",
    "                        pass\n",
    " \n",
    "    df = pd.DataFrame(rows, columns=[\n",
    "                      'cell', 'position', 'trial', 'ts'] + ([] if num_stimuli == 1 else ['ismatch']))\n",
    "    # df = df.set_index(['cell', 'position', 'trial'])\n",
    "    # , pd.DataFrame(cells, columns=['cell', 'monkey', 'area']).set_index('cell')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cells(df, style='monkey-area'):\n",
    "    cells = df.cell.unique()\n",
    "    cells = pd.DataFrame(cells, columns=['cell'])\n",
    "    cells['monkey'] = cells.cell.apply(lambda x: x.split('-')[0])\n",
    "    cells['area'] = cells.cell.apply(lambda x: x.split('-')[1])\n",
    "    if 'subregion' in style:\n",
    "        cells['subregion'] = cells.cell.apply(lambda x: x.split('-')[2])\n",
    "    if 'phase' in style:\n",
    "        cells['phase'] = cells.cell.apply(lambda x: x.split('-')[3])\n",
    "    cells = cells.set_index('cell')\n",
    "    return cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnm_spatial():\n",
    "    asd_mat = loadmat(\n",
    "        '../../../data/MNM_original_data/all_spatial_data.mat')['all_spatial_data']\n",
    "    asi_mat = loadmat(\n",
    "        '../../../data/MNM_original_data/all_spatial_info.mat')['all_spatial_info']\n",
    "\n",
    "    asd_mat = asd_mat[:, :8]\n",
    "    asi_mat = asi_mat[:, [0, 3, 4]]\n",
    "    extract = np.vectorize(lambda x: x[0])\n",
    "    asi_mat = extract(asi_mat)\n",
    "\n",
    "    asi_df = pd.DataFrame(asi_mat, columns=['monkey', 'phase', 'subregion'])\n",
    "    asi_df.monkey = asi_df.monkey.apply(lambda x: x[0:3].upper())\n",
    "    asi_df.subregion = asi_df.subregion.apply(lambda x: SUBREGIONS[x].upper())\n",
    "\n",
    "    asd_df = mat_to_df(asd_mat, '<REPLACE>', 'PFC',\n",
    "                        num_stimuli=2, numeric_cell=True)\n",
    "\n",
    "    df = pd.merge(asd_df, asi_df, left_on='cell', right_index=True)\n",
    "    df['cell'] = df.apply(lambda r: '-'.join([r.monkey, 'PFC',\n",
    "                          str(r.subregion), r.phase, str(r.cell)]), axis=1)\n",
    "    cells = get_cells(df, style='monkey-area-subregion-phase')\n",
    "    df = df.drop(columns=['monkey', 'subregion', 'phase'])\n",
    "\n",
    "    return {'raw_df': df.set_index(['cell', 'position', 'trial']), 'cells': cells}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = load_mnm_spatial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.DataFrame(df_dict['raw_df'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = pd.DataFrame(df_dict['cells'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the index to move 'cell', 'position', and 'trial' back to columns\n",
    "raw_df = raw_df.reset_index()\n",
    "\n",
    "# Verify the change by printing the columns\n",
    "# raw_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell</th>\n",
       "      <th>position</th>\n",
       "      <th>trial</th>\n",
       "      <th>ts</th>\n",
       "      <th>ismatch</th>\n",
       "      <th>monkey</th>\n",
       "      <th>area</th>\n",
       "      <th>subregion</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76803</th>\n",
       "      <td>ELV-PFC-MD-PRE-629</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[4.80585, 4.86795]</td>\n",
       "      <td>0</td>\n",
       "      <td>ELV</td>\n",
       "      <td>PFC</td>\n",
       "      <td>MD</td>\n",
       "      <td>PRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4087</th>\n",
       "      <td>ELV-PFC-AD-PRE-43</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>[2.097, 3.429925, 3.457325, 3.5123, 3.8211, 4....</td>\n",
       "      <td>0</td>\n",
       "      <td>ELV</td>\n",
       "      <td>PFC</td>\n",
       "      <td>AD</td>\n",
       "      <td>PRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262870</th>\n",
       "      <td>ELV-PFC-PV-POST-2345</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>[1.092075, 1.122025, 1.938975, 2.564275, 2.853...</td>\n",
       "      <td>0</td>\n",
       "      <td>ELV</td>\n",
       "      <td>PFC</td>\n",
       "      <td>PV</td>\n",
       "      <td>POST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        cell  position  trial  \\\n",
       "76803     ELV-PFC-MD-PRE-629         1      1   \n",
       "4087       ELV-PFC-AD-PRE-43         7     11   \n",
       "262870  ELV-PFC-PV-POST-2345         7      3   \n",
       "\n",
       "                                                       ts  ismatch monkey  \\\n",
       "76803                                  [4.80585, 4.86795]        0    ELV   \n",
       "4087    [2.097, 3.429925, 3.457325, 3.5123, 3.8211, 4....        0    ELV   \n",
       "262870  [1.092075, 1.122025, 1.938975, 2.564275, 2.853...        0    ELV   \n",
       "\n",
       "       area subregion phase  \n",
       "76803   PFC        MD   PRE  \n",
       "4087    PFC        AD   PRE  \n",
       "262870  PFC        PV  POST  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(raw_df, cells, on='cell', how='inner')\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = df.ts.apply(lambda x: len(x))\n",
    "# lengths.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df\n",
    "filtered_df['ts_length'] = filtered_df['ts'].apply(len)\n",
    "average_ts_length = filtered_df.groupby('cell')['ts_length'].mean().reset_index()\n",
    "sum_ts_length = filtered_df.groupby('cell')['ts_length'].sum().reset_index()\n",
    "use_cell = average_ts_length[(average_ts_length.ts_length >= 100) & (sum_ts_length.ts_length >= 500)].cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12980, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = filtered_df[filtered_df['cell'].isin(use_cell)]\n",
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6497, 10), (6483, 10))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df = filtered_df[filtered_df.phase == \"PRE\"]\n",
    "post_df = filtered_df[filtered_df.phase == \"POST\"]\n",
    "pre_df.shape, post_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_soft_one_hot(pos):\n",
    "    return torch.tensor(\n",
    "        [\n",
    "            0.6 if pos == x else (0.2 if abs(x - pos) ==\n",
    "                                  1 or abs(x - pos) == 7 else 0)\n",
    "            for x in range(1, 9)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def create_y(l, func):\n",
    "    return torch.hstack(\n",
    "        [\n",
    "            torch.stack([func(y) for y in l]),\n",
    "            torch.tensor(l).unsqueeze(1),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def window_data(df, NUM_OF_SAMPLES, WINDOW_STRIDE, WINDOW_WIDTH, NUM_POSITIONS, NUM_OF_CELLS):\n",
    "    df = df.sort_values(['cell', 'position']).reset_index(drop=True)\n",
    "\n",
    "    window_data = []\n",
    "    for start in np.arange(1, 3.5, WINDOW_STRIDE):\n",
    "        window_data.append(df.ts.apply(lambda x: (\n",
    "            (x >= start) & (x < start + WINDOW_WIDTH)).sum()).array)\n",
    "\n",
    "    window_d_t = torch.tensor(window_data).T\n",
    "    BINS = window_d_t.shape[1]\n",
    "\n",
    "    X = torch.zeros((NUM_OF_SAMPLES * NUM_POSITIONS,\n",
    "                    NUM_OF_CELLS * BINS), dtype=torch.float32)\n",
    "    y = []\n",
    "    for p in range(1, NUM_POSITIONS+1):\n",
    "        y += [p] * NUM_OF_SAMPLES\n",
    "        cells_idxs = df[df.position == p-1].groupby(\"cell\", sort=False).apply(\n",
    "            lambda x: x.index)\n",
    "        for i, cell_idxs in enumerate(cells_idxs):\n",
    "            idxs = np.random.choice(cell_idxs, NUM_OF_SAMPLES)\n",
    "            for j, idx in enumerate(idxs):\n",
    "                X[(p - 1) * NUM_OF_SAMPLES + j, i *\n",
    "                  BINS: (i + 1) * BINS] = window_d_t[idx]\n",
    "\n",
    "    return X, create_y(y, get_soft_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jv/2ybky5l94gl0__5flr8q9nq40000gn/T/ipykernel_33925/3765100678.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cells_idxs = df[df.position == p-1].groupby(\"cell\", sort=False).apply(\n",
      "/var/folders/jv/2ybky5l94gl0__5flr8q9nq40000gn/T/ipykernel_33925/3765100678.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cells_idxs = df[df.position == p-1].groupby(\"cell\", sort=False).apply(\n",
      "/var/folders/jv/2ybky5l94gl0__5flr8q9nq40000gn/T/ipykernel_33925/3765100678.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cells_idxs = df[df.position == p-1].groupby(\"cell\", sort=False).apply(\n",
      "/var/folders/jv/2ybky5l94gl0__5flr8q9nq40000gn/T/ipykernel_33925/3765100678.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cells_idxs = df[df.position == p-1].groupby(\"cell\", sort=False).apply(\n",
      "/var/folders/jv/2ybky5l94gl0__5flr8q9nq40000gn/T/ipykernel_33925/3765100678.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cells_idxs = df[df.position == p-1].groupby(\"cell\", sort=False).apply(\n",
      "/var/folders/jv/2ybky5l94gl0__5flr8q9nq40000gn/T/ipykernel_33925/3765100678.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cells_idxs = df[df.position == p-1].groupby(\"cell\", sort=False).apply(\n",
      "/var/folders/jv/2ybky5l94gl0__5flr8q9nq40000gn/T/ipykernel_33925/3765100678.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cells_idxs = df[df.position == p-1].groupby(\"cell\", sort=False).apply(\n",
      "/var/folders/jv/2ybky5l94gl0__5flr8q9nq40000gn/T/ipykernel_33925/3765100678.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cells_idxs = df[df.position == p-1].groupby(\"cell\", sort=False).apply(\n",
      "/var/folders/jv/2ybky5l94gl0__5flr8q9nq40000gn/T/ipykernel_33925/3765100678.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cells_idxs = df[df.position == p-1].groupby(\"cell\", sort=False).apply(\n",
      "/var/folders/jv/2ybky5l94gl0__5flr8q9nq40000gn/T/ipykernel_33925/3765100678.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cells_idxs = df[df.position == p-1].groupby(\"cell\", sort=False).apply(\n",
      "/var/folders/jv/2ybky5l94gl0__5flr8q9nq40000gn/T/ipykernel_33925/3765100678.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cells_idxs = df[df.position == p-1].groupby(\"cell\", sort=False).apply(\n",
      "/var/folders/jv/2ybky5l94gl0__5flr8q9nq40000gn/T/ipykernel_33925/3765100678.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cells_idxs = df[df.position == p-1].groupby(\"cell\", sort=False).apply(\n",
      "/var/folders/jv/2ybky5l94gl0__5flr8q9nq40000gn/T/ipykernel_33925/3765100678.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cells_idxs = df[df.position == p-1].groupby(\"cell\", sort=False).apply(\n",
      "/var/folders/jv/2ybky5l94gl0__5flr8q9nq40000gn/T/ipykernel_33925/3765100678.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cells_idxs = df[df.position == p-1].groupby(\"cell\", sort=False).apply(\n",
      "/var/folders/jv/2ybky5l94gl0__5flr8q9nq40000gn/T/ipykernel_33925/3765100678.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cells_idxs = df[df.position == p-1].groupby(\"cell\", sort=False).apply(\n",
      "/var/folders/jv/2ybky5l94gl0__5flr8q9nq40000gn/T/ipykernel_33925/3765100678.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cells_idxs = df[df.position == p-1].groupby(\"cell\", sort=False).apply(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "NUM_OF_CELLS = pre_df.cell.nunique()\n",
    "NUM_OF_SAMPLES = 40\n",
    "WINDOW_STRIDE = 0.4\n",
    "WINDOW_WIDTH = 0.4\n",
    "NUM_POSITIONS = 8\n",
    "\n",
    "train, test = train_test_split(\n",
    "    pre_df,\n",
    "    test_size=0.2,\n",
    "    stratify=pre_df[[\"cell\", \"position\"]],\n",
    ")\n",
    "\n",
    "# X, y = window_data(pre_df, NUM_OF_SAMPLES, WINDOW_STRIDE, WINDOW_WIDTH, NUM_POSITIONS, NUM_OF_CELLS)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True) \n",
    "\n",
    "X_train, y_train = window_data(train, NUM_OF_SAMPLES, WINDOW_STRIDE, WINDOW_WIDTH, NUM_POSITIONS, NUM_OF_CELLS)\n",
    "X_test, y_test = window_data(test, NUM_OF_SAMPLES, WINDOW_STRIDE, WINDOW_WIDTH, NUM_POSITIONS, NUM_OF_CELLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([320, 378])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([320, 9])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 3.5664, Accuracy: 21.88%\n",
      "Epoch [2/20], Loss: 1.6184, Accuracy: 39.69%\n",
      "Epoch [3/20], Loss: 0.9458, Accuracy: 68.12%\n",
      "Epoch [4/20], Loss: 0.5360, Accuracy: 86.56%\n",
      "Epoch [5/20], Loss: 0.3213, Accuracy: 94.38%\n",
      "Epoch [6/20], Loss: 0.1992, Accuracy: 96.88%\n",
      "Epoch [7/20], Loss: 0.1364, Accuracy: 98.44%\n",
      "Epoch [8/20], Loss: 0.0885, Accuracy: 99.69%\n",
      "Epoch [9/20], Loss: 0.0698, Accuracy: 99.69%\n",
      "Epoch [10/20], Loss: 0.0572, Accuracy: 100.00%\n",
      "Epoch [11/20], Loss: 0.0398, Accuracy: 100.00%\n",
      "Epoch [12/20], Loss: 0.0313, Accuracy: 100.00%\n",
      "Epoch [13/20], Loss: 0.0262, Accuracy: 100.00%\n",
      "Epoch [14/20], Loss: 0.0231, Accuracy: 100.00%\n",
      "Epoch [15/20], Loss: 0.0200, Accuracy: 100.00%\n",
      "Epoch [16/20], Loss: 0.0182, Accuracy: 100.00%\n",
      "Epoch [17/20], Loss: 0.0164, Accuracy: 100.00%\n",
      "Epoch [18/20], Loss: 0.0147, Accuracy: 100.00%\n",
      "Epoch [19/20], Loss: 0.0131, Accuracy: 100.00%\n",
      "Epoch [20/20], Loss: 0.0121, Accuracy: 100.00%\n",
      "Training Finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "y_train_labels = y_train[:, -1].long()\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "y_test_labels = y_test[:, -1].long()\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class SimpleFNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SimpleFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "num_classes = y_train_labels.max().item() + 1\n",
    "\n",
    "model = SimpleFNN(input_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "print('Training Finished.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.7863, Test Accuracy: 29.38%\n",
      "Final Test Loss: 2.7863, Final Test Accuracy: 29.38%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
    "    return avg_test_loss, accuracy\n",
    "\n",
    "# Evaluate the model after training\n",
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "\n",
    "print(f'Final Test Loss: {test_loss:.4f}, Final Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 1.2770, Accuracy: 23.44%\n",
      "Epoch [2/30], Loss: 0.9258, Accuracy: 46.88%\n",
      "Epoch [3/30], Loss: 0.7993, Accuracy: 50.00%\n",
      "Epoch [4/30], Loss: 0.7107, Accuracy: 57.81%\n",
      "Epoch [5/30], Loss: 0.7134, Accuracy: 53.12%\n",
      "Epoch [6/30], Loss: 0.6615, Accuracy: 57.81%\n",
      "Epoch [7/30], Loss: 0.6145, Accuracy: 71.88%\n",
      "Epoch [8/30], Loss: 0.5986, Accuracy: 68.75%\n",
      "Epoch [9/30], Loss: 0.5849, Accuracy: 78.12%\n",
      "Epoch [10/30], Loss: 0.5182, Accuracy: 79.69%\n",
      "Epoch [11/30], Loss: 0.4813, Accuracy: 79.69%\n",
      "Epoch [12/30], Loss: 0.4397, Accuracy: 85.94%\n",
      "Epoch [13/30], Loss: 0.4194, Accuracy: 87.50%\n",
      "Epoch [14/30], Loss: 0.3655, Accuracy: 89.06%\n",
      "Epoch [15/30], Loss: 0.3503, Accuracy: 84.38%\n",
      "Epoch [16/30], Loss: 0.3144, Accuracy: 92.19%\n",
      "Epoch [17/30], Loss: 0.2747, Accuracy: 90.62%\n",
      "Epoch [18/30], Loss: 0.2427, Accuracy: 93.75%\n",
      "Epoch [19/30], Loss: 0.2055, Accuracy: 95.31%\n",
      "Epoch [20/30], Loss: 0.1777, Accuracy: 96.88%\n",
      "Epoch [21/30], Loss: 0.1445, Accuracy: 98.44%\n",
      "Epoch [22/30], Loss: 0.1213, Accuracy: 98.44%\n",
      "Epoch [23/30], Loss: 0.0985, Accuracy: 100.00%\n",
      "Epoch [24/30], Loss: 0.0827, Accuracy: 100.00%\n",
      "Epoch [25/30], Loss: 0.0686, Accuracy: 100.00%\n",
      "Epoch [26/30], Loss: 0.0578, Accuracy: 100.00%\n",
      "Epoch [27/30], Loss: 0.0437, Accuracy: 100.00%\n",
      "Epoch [28/30], Loss: 0.0345, Accuracy: 100.00%\n",
      "Epoch [29/30], Loss: 0.0294, Accuracy: 100.00%\n",
      "Epoch [30/30], Loss: 0.0232, Accuracy: 100.00%\n",
      "Training Finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "y_train_labels = y_train[:, -1].long()\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "y_test_labels = y_test[:, -1].long()\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test_labels)\n",
    "test_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_cells, bins, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_cells, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "BINS = 7\n",
    "num_cells = X.shape[1] // BINS\n",
    "bins = BINS\n",
    "num_classes = y_train_labels.max().item() + 1\n",
    "\n",
    "model = SimpleCNN(num_cells, bins, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.view(inputs.size(0), num_cells, bins)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "print('Training Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0203, Test Accuracy: 100.00%\n",
      "Final Test Loss: 0.0203, Final Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, criterion, num_cells, bins):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.view(inputs.size(0), num_cells, bins)  # Reshape the inputs to match the CNN input size\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
    "    return avg_test_loss, accuracy\n",
    "\n",
    "# Evaluate the model after training\n",
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, num_cells, bins)\n",
    "\n",
    "print(f'Final Test Loss: {test_loss:.4f}, Final Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.1008, Accuracy: 51.56%\n",
      "Epoch [2/100], Loss: 1.0553, Accuracy: 51.56%\n",
      "Epoch [3/100], Loss: 1.0171, Accuracy: 51.56%\n",
      "Epoch [4/100], Loss: 0.9780, Accuracy: 51.56%\n",
      "Epoch [5/100], Loss: 0.9353, Accuracy: 51.56%\n",
      "Epoch [6/100], Loss: 0.8926, Accuracy: 51.56%\n",
      "Epoch [7/100], Loss: 0.8535, Accuracy: 51.56%\n",
      "Epoch [8/100], Loss: 0.8199, Accuracy: 51.56%\n",
      "Epoch [9/100], Loss: 0.7936, Accuracy: 51.56%\n",
      "Epoch [10/100], Loss: 0.7734, Accuracy: 51.56%\n",
      "Epoch [11/100], Loss: 0.7594, Accuracy: 51.56%\n",
      "Epoch [12/100], Loss: 0.7479, Accuracy: 51.56%\n",
      "Epoch [13/100], Loss: 0.7400, Accuracy: 59.38%\n",
      "Epoch [14/100], Loss: 0.7327, Accuracy: 62.50%\n",
      "Epoch [15/100], Loss: 0.7268, Accuracy: 57.81%\n",
      "Epoch [16/100], Loss: 0.7213, Accuracy: 57.81%\n",
      "Epoch [17/100], Loss: 0.7166, Accuracy: 60.94%\n",
      "Epoch [18/100], Loss: 0.7128, Accuracy: 53.12%\n",
      "Epoch [19/100], Loss: 0.7085, Accuracy: 54.69%\n",
      "Epoch [20/100], Loss: 0.7049, Accuracy: 57.81%\n",
      "Epoch [21/100], Loss: 0.7003, Accuracy: 59.38%\n",
      "Epoch [22/100], Loss: 0.6970, Accuracy: 73.44%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100], Loss: 0.6902, Accuracy: 70.31%\n",
      "Epoch [24/100], Loss: 0.6846, Accuracy: 70.31%\n",
      "Epoch [25/100], Loss: 0.6808, Accuracy: 70.31%\n",
      "Epoch [26/100], Loss: 0.6726, Accuracy: 71.88%\n",
      "Epoch [27/100], Loss: 0.6664, Accuracy: 73.44%\n",
      "Epoch [28/100], Loss: 0.6567, Accuracy: 73.44%\n",
      "Epoch [29/100], Loss: 0.6493, Accuracy: 79.69%\n",
      "Epoch [30/100], Loss: 0.6604, Accuracy: 64.06%\n",
      "Epoch [31/100], Loss: 0.6423, Accuracy: 75.00%\n",
      "Epoch [32/100], Loss: 0.6318, Accuracy: 79.69%\n",
      "Epoch [33/100], Loss: 0.6274, Accuracy: 71.88%\n",
      "Epoch [34/100], Loss: 0.6167, Accuracy: 78.12%\n",
      "Epoch [35/100], Loss: 0.6065, Accuracy: 82.81%\n",
      "Epoch [36/100], Loss: 0.5993, Accuracy: 78.12%\n",
      "Epoch [37/100], Loss: 0.5901, Accuracy: 75.00%\n",
      "Epoch [38/100], Loss: 0.5640, Accuracy: 84.38%\n",
      "Epoch [39/100], Loss: 0.5658, Accuracy: 84.38%\n",
      "Epoch [40/100], Loss: 0.5341, Accuracy: 84.38%\n",
      "Epoch [41/100], Loss: 0.5278, Accuracy: 87.50%\n",
      "Epoch [42/100], Loss: 0.5103, Accuracy: 87.50%\n",
      "Epoch [43/100], Loss: 0.4873, Accuracy: 92.19%\n",
      "Epoch [44/100], Loss: 0.4750, Accuracy: 90.62%\n",
      "Epoch [45/100], Loss: 0.4611, Accuracy: 90.62%\n",
      "Epoch [46/100], Loss: 0.4649, Accuracy: 85.94%\n",
      "Epoch [47/100], Loss: 0.4520, Accuracy: 90.62%\n",
      "Epoch [48/100], Loss: 0.4252, Accuracy: 92.19%\n",
      "Epoch [49/100], Loss: 0.4149, Accuracy: 93.75%\n",
      "Epoch [50/100], Loss: 0.3903, Accuracy: 92.19%\n",
      "Epoch [51/100], Loss: 0.3912, Accuracy: 92.19%\n",
      "Epoch [52/100], Loss: 0.3684, Accuracy: 93.75%\n",
      "Epoch [53/100], Loss: 0.3452, Accuracy: 96.88%\n",
      "Epoch [54/100], Loss: 0.3240, Accuracy: 95.31%\n",
      "Epoch [55/100], Loss: 0.2973, Accuracy: 96.88%\n",
      "Epoch [56/100], Loss: 0.2924, Accuracy: 96.88%\n",
      "Epoch [57/100], Loss: 0.2711, Accuracy: 98.44%\n",
      "Epoch [58/100], Loss: 0.2523, Accuracy: 100.00%\n",
      "Epoch [59/100], Loss: 0.2441, Accuracy: 100.00%\n",
      "Epoch [60/100], Loss: 0.2293, Accuracy: 100.00%\n",
      "Epoch [61/100], Loss: 0.2092, Accuracy: 100.00%\n",
      "Epoch [62/100], Loss: 0.1999, Accuracy: 100.00%\n",
      "Epoch [63/100], Loss: 0.1900, Accuracy: 100.00%\n",
      "Epoch [64/100], Loss: 0.1777, Accuracy: 100.00%\n",
      "Epoch [65/100], Loss: 0.1669, Accuracy: 100.00%\n",
      "Epoch [66/100], Loss: 0.1586, Accuracy: 100.00%\n",
      "Epoch [67/100], Loss: 0.1493, Accuracy: 100.00%\n",
      "Epoch [68/100], Loss: 0.1422, Accuracy: 100.00%\n",
      "Epoch [69/100], Loss: 0.1328, Accuracy: 100.00%\n",
      "Epoch [70/100], Loss: 0.1254, Accuracy: 100.00%\n",
      "Epoch [71/100], Loss: 0.1183, Accuracy: 100.00%\n",
      "Epoch [72/100], Loss: 0.1116, Accuracy: 100.00%\n",
      "Epoch [73/100], Loss: 0.1052, Accuracy: 100.00%\n",
      "Epoch [74/100], Loss: 0.0994, Accuracy: 100.00%\n",
      "Epoch [75/100], Loss: 0.0943, Accuracy: 100.00%\n",
      "Epoch [76/100], Loss: 0.0897, Accuracy: 100.00%\n",
      "Epoch [77/100], Loss: 0.0853, Accuracy: 100.00%\n",
      "Epoch [78/100], Loss: 0.0812, Accuracy: 100.00%\n",
      "Epoch [79/100], Loss: 0.0774, Accuracy: 100.00%\n",
      "Epoch [80/100], Loss: 0.0738, Accuracy: 100.00%\n",
      "Epoch [81/100], Loss: 0.0704, Accuracy: 100.00%\n",
      "Epoch [82/100], Loss: 0.0670, Accuracy: 100.00%\n",
      "Epoch [83/100], Loss: 0.0636, Accuracy: 100.00%\n",
      "Epoch [84/100], Loss: 0.0601, Accuracy: 100.00%\n",
      "Epoch [85/100], Loss: 0.0568, Accuracy: 100.00%\n",
      "Epoch [86/100], Loss: 0.0534, Accuracy: 100.00%\n",
      "Epoch [87/100], Loss: 0.0501, Accuracy: 100.00%\n",
      "Epoch [88/100], Loss: 0.0470, Accuracy: 100.00%\n",
      "Epoch [89/100], Loss: 0.0444, Accuracy: 100.00%\n",
      "Epoch [90/100], Loss: 0.0421, Accuracy: 100.00%\n",
      "Epoch [91/100], Loss: 0.0402, Accuracy: 100.00%\n",
      "Epoch [92/100], Loss: 0.0385, Accuracy: 100.00%\n",
      "Epoch [93/100], Loss: 0.0370, Accuracy: 100.00%\n",
      "Epoch [94/100], Loss: 0.0354, Accuracy: 100.00%\n",
      "Epoch [95/100], Loss: 0.0341, Accuracy: 100.00%\n",
      "Epoch [96/100], Loss: 0.0327, Accuracy: 100.00%\n",
      "Epoch [97/100], Loss: 0.0315, Accuracy: 100.00%\n",
      "Epoch [98/100], Loss: 0.0303, Accuracy: 100.00%\n",
      "Epoch [99/100], Loss: 0.0291, Accuracy: 100.00%\n",
      "Epoch [100/100], Loss: 0.0280, Accuracy: 100.00%\n",
      "Training Finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "y_train_labels = y_train[:, -1].long()\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "y_test_labels = y_test[:, -1].long()\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test_labels)\n",
    "test_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=num_features, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.relu(x[:, -1, :])\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "num_features = BINS\n",
    "sequence_length = len(pre_df.cell.unique())\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "num_classes = y_train_labels.max().item() + 1\n",
    "\n",
    "model = LSTMModel(num_features, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.view(inputs.size(0), sequence_length, num_features)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "print('Training Finished.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0272, Test Accuracy: 100.00%\n",
      "Final Test Loss: 0.0272, Final Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function for LSTM model\n",
    "def evaluate_model(model, test_loader, criterion, sequence_length, num_features):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.view(inputs.size(0), sequence_length, num_features)  # Reshape inputs\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
    "    return avg_test_loss, accuracy\n",
    "\n",
    "# Evaluate the model after training\n",
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, sequence_length, num_features)\n",
    "\n",
    "print(f'Final Test Loss: {test_loss:.4f}, Final Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "NUM_OF_CELLS = post_df.cell.nunique()\n",
    "NUM_OF_SAMPLES = 40\n",
    "WINDOW_STRIDE = 0.4\n",
    "WINDOW_WIDTH = 0.4\n",
    "NUM_POSITIONS = 2\n",
    "\n",
    "X, y = window_data(post_df, NUM_OF_SAMPLES, WINDOW_STRIDE, WINDOW_WIDTH, NUM_POSITIONS, NUM_OF_CELLS)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 5.0464, Accuracy: 43.75%\n",
      "Epoch [2/20], Loss: 2.2090, Accuracy: 54.69%\n",
      "Epoch [3/20], Loss: 1.9017, Accuracy: 51.56%\n",
      "Epoch [4/20], Loss: 0.4406, Accuracy: 68.75%\n",
      "Epoch [5/20], Loss: 0.9413, Accuracy: 59.38%\n",
      "Epoch [6/20], Loss: 0.2501, Accuracy: 90.62%\n",
      "Epoch [7/20], Loss: 0.4466, Accuracy: 78.12%\n",
      "Epoch [8/20], Loss: 0.2535, Accuracy: 89.06%\n",
      "Epoch [9/20], Loss: 0.0840, Accuracy: 96.88%\n",
      "Epoch [10/20], Loss: 0.1432, Accuracy: 93.75%\n",
      "Epoch [11/20], Loss: 0.1127, Accuracy: 95.31%\n",
      "Epoch [12/20], Loss: 0.0425, Accuracy: 100.00%\n",
      "Epoch [13/20], Loss: 0.0279, Accuracy: 100.00%\n",
      "Epoch [14/20], Loss: 0.0514, Accuracy: 98.44%\n",
      "Epoch [15/20], Loss: 0.0456, Accuracy: 100.00%\n",
      "Epoch [16/20], Loss: 0.0218, Accuracy: 100.00%\n",
      "Epoch [17/20], Loss: 0.0099, Accuracy: 100.00%\n",
      "Epoch [18/20], Loss: 0.0093, Accuracy: 100.00%\n",
      "Epoch [19/20], Loss: 0.0134, Accuracy: 100.00%\n",
      "Epoch [20/20], Loss: 0.0158, Accuracy: 100.00%\n",
      "Training Finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "y_train_labels = y_train[:, -1].long()\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "y_test_labels = y_test[:, -1].long()\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test_labels)\n",
    "test_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class SimpleFNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SimpleFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "input_size = X.shape[1]\n",
    "num_classes = y_train_labels.max().item() + 1\n",
    "\n",
    "model = SimpleFNN(input_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "print('Training Finished.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0146, Test Accuracy: 100.00%\n",
      "Final Test Loss: 0.0146, Final Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
    "    return avg_test_loss, accuracy\n",
    "\n",
    "# Evaluate the model after training\n",
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "\n",
    "print(f'Final Test Loss: {test_loss:.4f}, Final Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 1.1102, Accuracy: 23.44%\n",
      "Epoch [2/30], Loss: 0.8446, Accuracy: 50.00%\n",
      "Epoch [3/30], Loss: 0.7165, Accuracy: 62.50%\n",
      "Epoch [4/30], Loss: 0.6540, Accuracy: 68.75%\n",
      "Epoch [5/30], Loss: 0.6205, Accuracy: 68.75%\n",
      "Epoch [6/30], Loss: 0.5714, Accuracy: 84.38%\n",
      "Epoch [7/30], Loss: 0.5161, Accuracy: 82.81%\n",
      "Epoch [8/30], Loss: 0.4673, Accuracy: 89.06%\n",
      "Epoch [9/30], Loss: 0.3955, Accuracy: 92.19%\n",
      "Epoch [10/30], Loss: 0.3323, Accuracy: 95.31%\n",
      "Epoch [11/30], Loss: 0.2876, Accuracy: 95.31%\n",
      "Epoch [12/30], Loss: 0.2094, Accuracy: 96.88%\n",
      "Epoch [13/30], Loss: 0.1742, Accuracy: 98.44%\n",
      "Epoch [14/30], Loss: 0.1264, Accuracy: 96.88%\n",
      "Epoch [15/30], Loss: 0.1097, Accuracy: 98.44%\n",
      "Epoch [16/30], Loss: 0.0804, Accuracy: 100.00%\n",
      "Epoch [17/30], Loss: 0.0684, Accuracy: 98.44%\n",
      "Epoch [18/30], Loss: 0.0437, Accuracy: 100.00%\n",
      "Epoch [19/30], Loss: 0.0319, Accuracy: 100.00%\n",
      "Epoch [20/30], Loss: 0.0188, Accuracy: 100.00%\n",
      "Epoch [21/30], Loss: 0.0151, Accuracy: 100.00%\n",
      "Epoch [22/30], Loss: 0.0088, Accuracy: 100.00%\n",
      "Epoch [23/30], Loss: 0.0072, Accuracy: 100.00%\n",
      "Epoch [24/30], Loss: 0.0060, Accuracy: 100.00%\n",
      "Epoch [25/30], Loss: 0.0045, Accuracy: 100.00%\n",
      "Epoch [26/30], Loss: 0.0034, Accuracy: 100.00%\n",
      "Epoch [27/30], Loss: 0.0031, Accuracy: 100.00%\n",
      "Epoch [28/30], Loss: 0.0024, Accuracy: 100.00%\n",
      "Epoch [29/30], Loss: 0.0018, Accuracy: 100.00%\n",
      "Epoch [30/30], Loss: 0.0017, Accuracy: 100.00%\n",
      "Training Finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "y_train_labels = y_train[:, -1].long()\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "y_test_labels = y_test[:, -1].long()\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test_labels)\n",
    "test_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_cells, bins, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_cells, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "BINS = 7\n",
    "num_cells = X.shape[1] // BINS\n",
    "bins = BINS\n",
    "num_classes = y_train_labels.max().item() + 1\n",
    "\n",
    "model = SimpleCNN(num_cells, bins, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.view(inputs.size(0), num_cells, bins)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "print('Training Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0015, Test Accuracy: 100.00%\n",
      "Final Test Loss: 0.0015, Final Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, criterion, num_cells, bins):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.view(inputs.size(0), num_cells, bins)  # Reshape the inputs to match the CNN input size\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
    "    return avg_test_loss, accuracy\n",
    "\n",
    "# Evaluate the model after training\n",
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, num_cells, bins)\n",
    "\n",
    "print(f'Final Test Loss: {test_loss:.4f}, Final Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.0824, Accuracy: 43.75%\n",
      "Epoch [2/100], Loss: 1.0297, Accuracy: 45.31%\n",
      "Epoch [3/100], Loss: 0.9788, Accuracy: 51.56%\n",
      "Epoch [4/100], Loss: 0.9282, Accuracy: 51.56%\n",
      "Epoch [5/100], Loss: 0.8791, Accuracy: 51.56%\n",
      "Epoch [6/100], Loss: 0.8334, Accuracy: 51.56%\n",
      "Epoch [7/100], Loss: 0.8009, Accuracy: 51.56%\n",
      "Epoch [8/100], Loss: 0.7757, Accuracy: 51.56%\n",
      "Epoch [9/100], Loss: 0.7583, Accuracy: 51.56%\n",
      "Epoch [10/100], Loss: 0.7445, Accuracy: 51.56%\n",
      "Epoch [11/100], Loss: 0.7326, Accuracy: 51.56%\n",
      "Epoch [12/100], Loss: 0.7238, Accuracy: 51.56%\n",
      "Epoch [13/100], Loss: 0.7168, Accuracy: 51.56%\n",
      "Epoch [14/100], Loss: 0.7118, Accuracy: 59.38%\n",
      "Epoch [15/100], Loss: 0.7058, Accuracy: 65.62%\n",
      "Epoch [16/100], Loss: 0.7016, Accuracy: 73.44%\n",
      "Epoch [17/100], Loss: 0.6947, Accuracy: 68.75%\n",
      "Epoch [18/100], Loss: 0.6885, Accuracy: 73.44%\n",
      "Epoch [19/100], Loss: 0.6821, Accuracy: 73.44%\n",
      "Epoch [20/100], Loss: 0.6755, Accuracy: 75.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100], Loss: 0.6660, Accuracy: 75.00%\n",
      "Epoch [22/100], Loss: 0.6584, Accuracy: 78.12%\n",
      "Epoch [23/100], Loss: 0.6477, Accuracy: 73.44%\n",
      "Epoch [24/100], Loss: 0.6403, Accuracy: 78.12%\n",
      "Epoch [25/100], Loss: 0.6302, Accuracy: 79.69%\n",
      "Epoch [26/100], Loss: 0.6197, Accuracy: 76.56%\n",
      "Epoch [27/100], Loss: 0.6040, Accuracy: 81.25%\n",
      "Epoch [28/100], Loss: 0.5903, Accuracy: 81.25%\n",
      "Epoch [29/100], Loss: 0.5759, Accuracy: 81.25%\n",
      "Epoch [30/100], Loss: 0.5657, Accuracy: 82.81%\n",
      "Epoch [31/100], Loss: 0.5463, Accuracy: 85.94%\n",
      "Epoch [32/100], Loss: 0.5334, Accuracy: 82.81%\n",
      "Epoch [33/100], Loss: 0.5230, Accuracy: 82.81%\n",
      "Epoch [34/100], Loss: 0.5025, Accuracy: 85.94%\n",
      "Epoch [35/100], Loss: 0.4892, Accuracy: 84.38%\n",
      "Epoch [36/100], Loss: 0.4708, Accuracy: 85.94%\n",
      "Epoch [37/100], Loss: 0.4549, Accuracy: 87.50%\n",
      "Epoch [38/100], Loss: 0.4343, Accuracy: 87.50%\n",
      "Epoch [39/100], Loss: 0.4232, Accuracy: 89.06%\n",
      "Epoch [40/100], Loss: 0.4037, Accuracy: 89.06%\n",
      "Epoch [41/100], Loss: 0.3878, Accuracy: 89.06%\n",
      "Epoch [42/100], Loss: 0.3773, Accuracy: 92.19%\n",
      "Epoch [43/100], Loss: 0.3543, Accuracy: 92.19%\n",
      "Epoch [44/100], Loss: 0.3463, Accuracy: 89.06%\n",
      "Epoch [45/100], Loss: 0.3138, Accuracy: 93.75%\n",
      "Epoch [46/100], Loss: 0.2981, Accuracy: 96.88%\n",
      "Epoch [47/100], Loss: 0.2771, Accuracy: 98.44%\n",
      "Epoch [48/100], Loss: 0.2581, Accuracy: 98.44%\n",
      "Epoch [49/100], Loss: 0.2416, Accuracy: 98.44%\n",
      "Epoch [50/100], Loss: 0.2243, Accuracy: 98.44%\n",
      "Epoch [51/100], Loss: 0.2082, Accuracy: 98.44%\n",
      "Epoch [52/100], Loss: 0.1922, Accuracy: 100.00%\n",
      "Epoch [53/100], Loss: 0.1801, Accuracy: 100.00%\n",
      "Epoch [54/100], Loss: 0.1663, Accuracy: 100.00%\n",
      "Epoch [55/100], Loss: 0.1567, Accuracy: 100.00%\n",
      "Epoch [56/100], Loss: 0.1453, Accuracy: 100.00%\n",
      "Epoch [57/100], Loss: 0.1356, Accuracy: 100.00%\n",
      "Epoch [58/100], Loss: 0.1250, Accuracy: 100.00%\n",
      "Epoch [59/100], Loss: 0.1175, Accuracy: 100.00%\n",
      "Epoch [60/100], Loss: 0.1099, Accuracy: 100.00%\n",
      "Epoch [61/100], Loss: 0.1032, Accuracy: 100.00%\n",
      "Epoch [62/100], Loss: 0.0975, Accuracy: 100.00%\n",
      "Epoch [63/100], Loss: 0.0919, Accuracy: 100.00%\n",
      "Epoch [64/100], Loss: 0.0861, Accuracy: 100.00%\n",
      "Epoch [65/100], Loss: 0.0814, Accuracy: 100.00%\n",
      "Epoch [66/100], Loss: 0.0765, Accuracy: 100.00%\n",
      "Epoch [67/100], Loss: 0.0722, Accuracy: 100.00%\n",
      "Epoch [68/100], Loss: 0.0679, Accuracy: 100.00%\n",
      "Epoch [69/100], Loss: 0.0639, Accuracy: 100.00%\n",
      "Epoch [70/100], Loss: 0.0599, Accuracy: 100.00%\n",
      "Epoch [71/100], Loss: 0.0560, Accuracy: 100.00%\n",
      "Epoch [72/100], Loss: 0.0527, Accuracy: 100.00%\n",
      "Epoch [73/100], Loss: 0.0495, Accuracy: 100.00%\n",
      "Epoch [74/100], Loss: 0.0466, Accuracy: 100.00%\n",
      "Epoch [75/100], Loss: 0.0437, Accuracy: 100.00%\n",
      "Epoch [76/100], Loss: 0.0409, Accuracy: 100.00%\n",
      "Epoch [77/100], Loss: 0.0384, Accuracy: 100.00%\n",
      "Epoch [78/100], Loss: 0.0360, Accuracy: 100.00%\n",
      "Epoch [79/100], Loss: 0.0340, Accuracy: 100.00%\n",
      "Epoch [80/100], Loss: 0.0322, Accuracy: 100.00%\n",
      "Epoch [81/100], Loss: 0.0305, Accuracy: 100.00%\n",
      "Epoch [82/100], Loss: 0.0289, Accuracy: 100.00%\n",
      "Epoch [83/100], Loss: 0.0275, Accuracy: 100.00%\n",
      "Epoch [84/100], Loss: 0.0264, Accuracy: 100.00%\n",
      "Epoch [85/100], Loss: 0.0251, Accuracy: 100.00%\n",
      "Epoch [86/100], Loss: 0.0240, Accuracy: 100.00%\n",
      "Epoch [87/100], Loss: 0.0231, Accuracy: 100.00%\n",
      "Epoch [88/100], Loss: 0.0221, Accuracy: 100.00%\n",
      "Epoch [89/100], Loss: 0.0212, Accuracy: 100.00%\n",
      "Epoch [90/100], Loss: 0.0204, Accuracy: 100.00%\n",
      "Epoch [91/100], Loss: 0.0197, Accuracy: 100.00%\n",
      "Epoch [92/100], Loss: 0.0189, Accuracy: 100.00%\n",
      "Epoch [93/100], Loss: 0.0183, Accuracy: 100.00%\n",
      "Epoch [94/100], Loss: 0.0176, Accuracy: 100.00%\n",
      "Epoch [95/100], Loss: 0.0170, Accuracy: 100.00%\n",
      "Epoch [96/100], Loss: 0.0165, Accuracy: 100.00%\n",
      "Epoch [97/100], Loss: 0.0159, Accuracy: 100.00%\n",
      "Epoch [98/100], Loss: 0.0155, Accuracy: 100.00%\n",
      "Epoch [99/100], Loss: 0.0149, Accuracy: 100.00%\n",
      "Epoch [100/100], Loss: 0.0144, Accuracy: 100.00%\n",
      "Training Finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "y_train_labels = y_train[:, -1].long()\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "y_test_labels = y_test[:, -1].long()\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test_labels)\n",
    "test_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=num_features, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.relu(x[:, -1, :])\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "num_features = BINS\n",
    "sequence_length = len(post_df.cell.unique())\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "num_classes = y_train_labels.max().item() + 1\n",
    "\n",
    "model = LSTMModel(num_features, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.view(inputs.size(0), sequence_length, num_features)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "print('Training Finished.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0140, Test Accuracy: 100.00%\n",
      "Final Test Loss: 0.0140, Final Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function for LSTM model\n",
    "def evaluate_model(model, test_loader, criterion, sequence_length, num_features):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.view(inputs.size(0), sequence_length, num_features)  # Reshape inputs\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
    "    return avg_test_loss, accuracy\n",
    "\n",
    "# Evaluate the model after training\n",
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, sequence_length, num_features)\n",
    "\n",
    "print(f'Final Test Loss: {test_loss:.4f}, Final Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
